{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 环境配置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [
    {
     "data": {
      "text/plain": "'1.12.1+cpu'"
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "data": {
      "text/plain": "'4.21.2'"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 全局变量"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "max_length = 128\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# 每${log_after_step}步，打印一次日志\n",
    "log_after_step = 20\n",
    "\n",
    "# 模型存放的位置\n",
    "model_path = './drive/MyDrive/models/MDCSpell/'\n",
    "os.mkdir(model_path) if not os.path.exists(model_path) else ''\n",
    "model_path = model_path + 'MDCSpell-model.pt'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型构建"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "class CorrectionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CorrectionNetwork, self).__init__()\n",
    "        # BERT分词器\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "        # BERT\n",
    "        self.bert = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "        # BERT的word embedding，本质就是个nn.Embedding\n",
    "        self.word_embedding_table = self.bert.get_input_embeddings()\n",
    "        # 预测层。hidden_size是词向量的大小，len(self.tokenizer)是词典大小\n",
    "        self.dense_layer = nn.Linear(self.bert.config.hidden_size, len(self.tokenizer))\n",
    "\n",
    "    def forward(self, inputs, word_embeddings, detect_hidden_states):\n",
    "        \"\"\"\n",
    "        Correction Network的前向传递\n",
    "        :param inputs: inputs为tokenizer对中文文本的分词结果，\n",
    "                       里面包含了token对一个的index，attention_mask等\n",
    "        :param word_embeddings: 使用BERT的word_embedding对token进行embedding后的结果\n",
    "        :param detect_hidden_states: Detection Network输出hidden state\n",
    "        :return: Correction Network对个token的预测结果。\n",
    "        \"\"\"\n",
    "        # 1. 使用bert进行前向传递\n",
    "        bert_outputs = self.bert(token_type_ids=inputs['token_type_ids'],\n",
    "                                 attention_mask=inputs['attention_mask'],\n",
    "                                 inputs_embeds=word_embeddings)\n",
    "        # 2. 将bert的hidden_state和Detection Network的hidden state进行融合。\n",
    "        hidden_states = bert_outputs['last_hidden_state'] + detect_hidden_states\n",
    "        # 3. 最终使用全连接层进行token预测\n",
    "        return self.dense_layer(hidden_states)\n",
    "\n",
    "    def get_inputs_and_word_embeddings(self, sequences, max_length=128):\n",
    "        \"\"\"\n",
    "        对中文序列进行分词和word embeddings处理\n",
    "        :param sequences: 中文文本序列。例如: [\"鸡你太美\", \"哎呦，你干嘛！\"]\n",
    "        :param max_length: 文本的最大长度，不足则进行填充，超出进行裁剪。\n",
    "        :return: tokenizer的输出和word embeddings.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(sequences, padding='max_length', max_length=max_length, return_tensors='pt',\n",
    "                                truncation=True).to(device)\n",
    "        # 使用BERT的work embeddings对token进行embedding，这里得到的embedding并不包含position embedding和segment embedding\n",
    "        word_embeddings = self.word_embedding_table(inputs['input_ids'])\n",
    "        return inputs, word_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "class DetectionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, position_embeddings, transformer_blocks, hidden_size):\n",
    "        \"\"\"\n",
    "        :param position_embeddings: bert的position_embeddings，本质是一个nn.Embedding\n",
    "        :param transformer: BERT的前两层transformer_block，其是一个ModuleList对象\n",
    "        \"\"\"\n",
    "        super(DetectionNetwork, self).__init__()\n",
    "        self.position_embeddings = position_embeddings\n",
    "        self.transformer_blocks = transformer_blocks\n",
    "\n",
    "        # 定义最后的预测层，预测哪个token是错误的\n",
    "        self.dense_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        # 获取token序列的长度，这里为128\n",
    "        sequence_length = word_embeddings.size(1)\n",
    "        # 生成position embedding\n",
    "        position_embeddings = self.position_embeddings(torch.LongTensor(range(sequence_length)).to(device))\n",
    "        # 融合work_embedding和position_embedding\n",
    "        x = word_embeddings + position_embeddings\n",
    "        # 将x一层一层的使用transformer encoder进行向后传递\n",
    "        for transformer_layer in self.transformer_blocks:\n",
    "            x = transformer_layer(x)[0]\n",
    "\n",
    "        # 最终返回Detection Network输出的hidden states和预测结果\n",
    "        hidden_states = x\n",
    "        return hidden_states, self.dense_layer(hidden_states)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "class MDCSpellModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MDCSpellModel, self).__init__()\n",
    "        # 构造Correction Network\n",
    "        self.correction_network = CorrectionNetwork()\n",
    "        self._init_correction_dense_layer()\n",
    "\n",
    "        # 构造Detection Network\n",
    "        # position embedding使用BERT的\n",
    "        position_embeddings = self.correction_network.bert.embeddings.position_embeddings\n",
    "        # 作者在论文中提到的，Detection Network的Transformer使用BERT的权重\n",
    "        # 所以我这里直接克隆BERT的前两层Transformer来完成这个动作\n",
    "        transformer = copy.deepcopy(self.correction_network.bert.encoder.layer[:2])\n",
    "        # 提取BERT的词向量大小\n",
    "        hidden_size = self.correction_network.bert.config.hidden_size\n",
    "\n",
    "        # 构造Detection Network\n",
    "        self.detection_network = DetectionNetwork(position_embeddings, transformer, hidden_size)\n",
    "\n",
    "    def forward(self, sequences, max_length=128):\n",
    "        # 先获取word embedding，Correction Network和Detection Network都要用\n",
    "        inputs, word_embeddings = self.correction_network.get_inputs_and_word_embeddings(sequences, max_length)\n",
    "        # Detection Network进行前向传递，获取输出的Hidden State和预测结果\n",
    "        hidden_states, detection_outputs = self.detection_network(word_embeddings)\n",
    "        # Correction Network进行前向传递，获取其预测结果\n",
    "        correction_outputs = self.correction_network(inputs, word_embeddings, hidden_states)\n",
    "        # 返回Correction Network 和 Detection Network 的预测结果。\n",
    "        # 在计算损失时`[PAD]`token不需要参与计算，所以这里将`[PAD]`部分全都变为0\n",
    "        return correction_outputs, detection_outputs.squeeze(2) * inputs['attention_mask']\n",
    "\n",
    "    def _init_correction_dense_layer(self):\n",
    "        \"\"\"\n",
    "        原论文中提到，使用Word Embedding的weight来对Correction Network进行初始化\n",
    "        \"\"\"\n",
    "        self.correction_network.dense_layer.weight.data = self.correction_network.word_embedding_table.weight.data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义好模型后，我们来简单的尝试一下："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction_outputs shape: torch.Size([2, 128, 21128])\n",
      "detection_outputs shape: torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "model = MDCSpellModel().to(device)\n",
    "correction_outputs, detection_outputs = model([\"鸡你太美\", \"哎呦，你干嘛！\"])\n",
    "print(\"correction_outputs shape:\", correction_outputs.size())\n",
    "print(\"detection_outputs shape:\", detection_outputs.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 损失函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "class MDCSpellLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, coefficient=0.85):\n",
    "        super(MDCSpellLoss, self).__init__()\n",
    "        # 定义Correction Network的Loss函数\n",
    "        self.correction_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        # 定义Detection Network的Loss函数，因为是二分类，所以用Binary Cross Entropy\n",
    "        self.detection_criterion = nn.BCELoss()\n",
    "        # 权重系数\n",
    "        self.coefficient = coefficient\n",
    "\n",
    "    def forward(self, correction_outputs, correction_targets, detection_outputs, detection_targets):\n",
    "        \"\"\"\n",
    "        :param correction_outputs: Correction Network的输出，Shape为(batch_size, sequence_length, hidden_size)\n",
    "        :param correction_targets: Correction Network的标签，Shape为(batch_size, sequence_length)\n",
    "        :param detection_outputs: Detection Network的输出，Shape为(batch_size, sequence_length)\n",
    "        :param detection_targets: Detection Network的标签，Shape为(batch_size, sequence_length)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 计算Correction Network的loss，因为Shape维度为3，所以要把batch_size和sequence_length进行合并才能计算\n",
    "        correction_loss = self.correction_criterion(correction_outputs.view(-1, correction_outputs.size(2)),\n",
    "                                                    correction_targets.view(-1))\n",
    "        # 计算Detection Network的loss\n",
    "        detection_loss = self.detection_criterion(detection_outputs, detection_targets)\n",
    "        # 对两个loss进行加权平均\n",
    "        return self.coefficient * correction_loss + (1 - self.coefficient) * detection_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "作者首先使用了Wang271K数据集进行对模型进行训练，然后又使用SIGHAN训练集对模型进行fine-tune。这里我就不进行fine-tune了，直接进行训练。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'gdown' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "!gdown '1dC09i57lobL91lEbpebDuUBS0fGz-LAk' --folder --output data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 构造Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "class CSCDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CSCDataset, self).__init__()\n",
    "        with open(\"data/trainall.times2.pkl\", mode='br') as f:\n",
    "            train_data = pickle.load(f)\n",
    "\n",
    "        self.train_data = train_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        src = self.train_data[index]['src']\n",
    "        tgt = self.train_data[index]['tgt']\n",
    "        return src, tgt\n",
    "\n",
    "    def __len__(self):\n",
    "        return 32\n",
    "        # return len(self.train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "train_data = CSCDataset()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "('纽约早盘作为基准的低硫轻油，五月份交割价攀升一点三四美元，来到每桶二十八点二五美元，而上周五曾下挫一豪元以上。',\n '纽约早盘作为基准的低硫轻油，五月份交割价攀升一点三四美元，来到每桶二十八点二五美元，而上周五曾下挫一美元以上。')"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.__getitem__(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 构造Dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    src, tgt = zip(*batch)\n",
    "    src, tgt = list(src), list(tgt)\n",
    "\n",
    "    src_tokens = tokenizer(src, padding='max_length', max_length=128, return_tensors='pt', truncation=True)['input_ids']\n",
    "    tgt_tokens = tokenizer(tgt, padding='max_length', max_length=128, return_tensors='pt', truncation=True)['input_ids']\n",
    "\n",
    "    correction_targets = tgt_tokens\n",
    "    detection_targets = (src_tokens != tgt_tokens).float()\n",
    "    return src, correction_targets, detection_targets, src_tokens  # src_tokens在计算Correction的精准率时要用到"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "criterion = MDCSpellLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "start_epoch = 0  # 从哪个epoch开始\n",
    "total_step = 0  # 一共更新了多少次参数\n",
    "record = []  # 记录loss、accuracy变化等"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "恢复训练，epoch: 1\n"
     ]
    }
   ],
   "source": [
    "# 恢复之前的训练\n",
    "if os.path.exists(model_path):\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    total_step = checkpoint['total_step']\n",
    "    record = checkpoint['record']\n",
    "    print(\"恢复训练，epoch:\", start_epoch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model = model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "log_after_step = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 1/1, Total Step 1, loss 0.16050, detection recall 0.0000, detection precision 0.0000, correction recall 0.0270, correction precision 0.0909\n",
      "Epoch 1, Step 1/1, Total Step 2, loss 0.13872, detection recall 0.0000, detection precision 0.0000, correction recall 0.0270, correction precision 0.0909\n",
      "Epoch 2, Step 1/1, Total Step 3, loss 0.12604, detection recall 0.0000, detection precision 0.0000, correction recall 0.0270, correction precision 0.0714\n",
      "Epoch 3, Step 1/1, Total Step 4, loss 0.10877, detection recall 0.0000, detection precision 0.0000, correction recall 0.0270, correction precision 0.3333\n",
      "Epoch 4, Step 1/1, Total Step 5, loss 0.09185, detection recall 0.0000, detection precision 0.0000, correction recall 0.1081, correction precision 0.4444\n",
      "Epoch 5, Step 1/1, Total Step 6, loss 0.07909, detection recall 0.0000, detection precision 0.0000, correction recall 0.2432, correction precision 0.7500\n",
      "Epoch 6, Step 1/1, Total Step 7, loss 0.06916, detection recall 0.0000, detection precision 0.0000, correction recall 0.4324, correction precision 0.8421\n",
      "Epoch 7, Step 1/1, Total Step 8, loss 0.06047, detection recall 0.0000, detection precision 0.0000, correction recall 0.5405, correction precision 0.8696\n",
      "Epoch 8, Step 1/1, Total Step 9, loss 0.05088, detection recall 0.0000, detection precision 0.0000, correction recall 0.5946, correction precision 0.9565\n",
      "Epoch 9, Step 1/1, Total Step 10, loss 0.04333, detection recall 0.0000, detection precision 0.0000, correction recall 0.7568, correction precision 1.0000\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0.  # 记录loss\n",
    "\n",
    "d_recall_numerator = 0  # Detection的Recall的分子\n",
    "d_recall_denominator = 0  # Detection的Recall的分母\n",
    "d_precision_numerator = 0  # Detection的precision的分子\n",
    "d_precision_denominator = 0  # Detection的precision的分母\n",
    "c_recall_numerator = 0  # Correction的Recall的分子\n",
    "c_recall_denominator = 0  # Correction的Recall的分母\n",
    "c_precision_numerator = 0  # Correction的precision的分子\n",
    "c_precision_denominator = 0  # Correction的precision的分母\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    for sequences, correction_targets, detection_targets, correction_inputs in train_loader:\n",
    "        correction_targets, detection_targets = correction_targets.to(device), detection_targets.to(device)\n",
    "        correction_outputs, detection_outputs = model(sequences)\n",
    "        loss = criterion(correction_outputs, correction_targets, detection_outputs, detection_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        step += 1\n",
    "        total_step += 1\n",
    "\n",
    "        total_loss += loss.detach().item()\n",
    "\n",
    "        # 计算Detection的recall和precision指标\n",
    "        # 大于0.5，认为是错误token，反之为正确token\n",
    "        d_predicts = detection_outputs >= 0.5\n",
    "        # 计算错误token中被网络正确预测到的数量\n",
    "        d_recall_numerator += d_predicts[detection_targets == 1].sum().item()\n",
    "        # 计算错误token的数量\n",
    "        d_recall_denominator += (detection_targets == 1).sum().item()\n",
    "        # 计算网络预测的错误token的数量\n",
    "        d_precision_denominator += d_predicts.sum().item()\n",
    "        # 计算网络预测的错误token中，有多少是真错误的token\n",
    "        d_precision_numerator += (detection_targets[d_predicts == 1]).sum().item()\n",
    "\n",
    "        # 计算Correction的recall和precision\n",
    "        # 将输出映射成index，即将correction_outputs的Shape由(32, 128, 21128)变为(32,128)\n",
    "        correction_outputs = correction_outputs.argmax(2)\n",
    "        # 对于填充、[CLS]和[SEP]这三个token不校验\n",
    "        correction_outputs[(correction_targets == 0) | (correction_targets == 101) | (correction_targets == 102)] = 0\n",
    "        # correction_targets的[CLS]和[SEP]也要变为0\n",
    "        correction_targets[(correction_targets == 101) | (correction_targets == 102)] = 0\n",
    "        # Correction的预测结果，其中True表示预测正确，False表示预测错误或无需预测\n",
    "        c_predicts = correction_outputs == correction_targets\n",
    "        # 计算错误token中被网络正确纠正的token数量\n",
    "        c_recall_numerator = c_predicts[detection_targets == 1].sum().item()\n",
    "        # 计算错误token的数量\n",
    "        c_recall_denominator = d_recall_denominator\n",
    "        # 计算网络纠正token的数量\n",
    "        correction_inputs[(correction_inputs == 101) | (correction_inputs == 102)] = 0\n",
    "        c_precision_denominator = (correction_outputs != correction_inputs).sum().item()\n",
    "        # 计算在网络纠正的这些token中，有多少是真正被纠正对的\n",
    "        c_precision_numerator = c_predicts[correction_outputs != correction_inputs].sum().item()\n",
    "\n",
    "        if total_step % log_after_step == 0:\n",
    "            loss = total_loss / log_after_step\n",
    "            d_recall = d_recall_numerator / (d_recall_denominator + 1e-9)\n",
    "            d_precision = d_precision_numerator / (d_precision_denominator + 1e-9)\n",
    "            c_recall = c_recall_numerator / (c_recall_denominator + 1e-9)\n",
    "            c_precision = c_precision_numerator / (c_precision_denominator + 1e-9)\n",
    "\n",
    "            print(\"Epoch {}, \"\n",
    "                  \"Step {}/{}, \"\n",
    "                  \"Total Step {}, \"\n",
    "                  \"loss {:.5f}, \"\n",
    "                  \"detection recall {:.4f}, \"\n",
    "                  \"detection precision {:.4f}, \"\n",
    "                  \"correction recall {:.4f}, \"\n",
    "                  \"correction precision {:.4f}\".format(epoch, step, len(train_loader), total_step,\n",
    "                                                loss,\n",
    "                                                d_recall,\n",
    "                                                d_precision,\n",
    "                                                c_recall,\n",
    "                                                c_precision))\n",
    "\n",
    "            record.append((total_step, loss, d_recall, d_precision,))\n",
    "\n",
    "            total_loss = 0.\n",
    "            total_correct = 0\n",
    "            total_num = 0\n",
    "            d_recall_numerator = 0\n",
    "            d_recall_denominator = 0\n",
    "            d_precision_numerator = 0\n",
    "            d_precision_denominator = 0\n",
    "\n",
    "\n",
    "    torch.save({\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'epoch': epoch + 1,\n",
    "        'total_step': total_step,\n",
    "        'record': record\n",
    "    }, model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}