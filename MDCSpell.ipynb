{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from transformers import AutoModel, AutoTokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型构建"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "class CorrectionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CorrectionNetwork, self).__init__()\n",
    "        # BERT分词器\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "        # BERT\n",
    "        self.bert = AutoModel.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "        # BERT的word embedding，本质就是个nn.Embedding\n",
    "        self.word_embedding_table = self.bert.get_input_embeddings()\n",
    "        # 预测层。hidden_size是词向量的大小，len(self.tokenizer)是词典大小\n",
    "        self.dense_layer = nn.Linear(self.bert.config.hidden_size, len(self.tokenizer))\n",
    "\n",
    "    def forward(self, inputs, word_embeddings, detect_hidden_states):\n",
    "        \"\"\"\n",
    "        Correction Network的前向传递\n",
    "        :param inputs: inputs为tokenizer对中文文本的分词结果，\n",
    "                       里面包含了token对一个的index，attention_mask等\n",
    "        :param word_embeddings: 使用BERT的word_embedding对token进行embedding后的结果\n",
    "        :param detect_hidden_states: Detection Network输出hidden state\n",
    "        :return: Correction Network对个token的预测结果。\n",
    "        \"\"\"\n",
    "        # 1. 使用bert进行前向传递\n",
    "        bert_outputs = self.bert(token_type_ids=inputs['token_type_ids'],\n",
    "                                 attention_mask=inputs['attention_mask'],\n",
    "                                 inputs_embeds=word_embeddings)\n",
    "        # 2. 将bert的hidden_state和Detection Network的hidden state进行融合。\n",
    "        hidden_states = bert_outputs['last_hidden_state'] + detect_hidden_states\n",
    "        # 3. 最终使用全连接层进行token预测\n",
    "        return self.dense_layer(hidden_states)\n",
    "\n",
    "    def get_inputs_and_word_embeddings(self, sequences, max_length=128):\n",
    "        \"\"\"\n",
    "        对中文序列进行分词和word embeddings处理\n",
    "        :param sequences: 中文文本序列。例如: [\"鸡你太美\", \"哎呦，你干嘛！\"]\n",
    "        :param max_length: 文本的最大长度，不足则进行填充，超出进行裁剪。\n",
    "        :return: tokenizer的输出和word embeddings.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(sequences, padding='max_length', max_length=max_length, return_tensors='pt',\n",
    "                                truncation=True)\n",
    "        # 使用BERT的work embeddings对token进行embedding，这里得到的embedding并不包含position embedding和segment embedding\n",
    "        word_embeddings = self.word_embedding_table(inputs['input_ids'])\n",
    "        return inputs, word_embeddings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "outputs": [],
   "source": [
    "class DetectionNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, position_embeddings, transformer_blocks, hidden_size):\n",
    "        \"\"\"\n",
    "        :param position_embeddings: bert的position_embeddings，本质是一个nn.Embedding\n",
    "        :param transformer: BERT的前两层transformer_block，其是一个ModuleList对象\n",
    "        \"\"\"\n",
    "        super(DetectionNetwork, self).__init__()\n",
    "        self.position_embeddings = position_embeddings\n",
    "        self.transformer_blocks = transformer_blocks\n",
    "\n",
    "        # 定义最后的预测层，预测哪个token是错误的\n",
    "        self.dense_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, word_embeddings):\n",
    "        # 获取token序列的长度，这里为128\n",
    "        sequence_length = word_embeddings.size(1)\n",
    "        # 生成position embedding\n",
    "        position_embeddings = self.position_embeddings(torch.LongTensor(range(sequence_length)))\n",
    "        # 融合work_embedding和position_embedding\n",
    "        x = word_embeddings + position_embeddings\n",
    "        # 将x一层一层的使用transformer encoder进行向后传递\n",
    "        for transformer_layer in self.transformer_blocks:\n",
    "            x = transformer_layer(x)[0]\n",
    "\n",
    "        # 最终返回Detection Network输出的hidden states和预测结果\n",
    "        hidden_states = x\n",
    "        return hidden_states, self.dense_layer(hidden_states)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "class MDCSpellModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MDCSpellModel, self).__init__()\n",
    "        # 构造Correction Network\n",
    "        self.correction_network = CorrectionNetwork()\n",
    "        self._init_correction_dense_layer()\n",
    "\n",
    "        # 构造Detection Network\n",
    "        # position embedding使用BERT的\n",
    "        position_embeddings = self.correction_network.bert.embeddings.position_embeddings\n",
    "        # 作者在论文中提到的，Detection Network的Transformer使用BERT的权重\n",
    "        # 所以我这里直接克隆BERT的前两层Transformer来完成这个动作\n",
    "        transformer = copy.deepcopy(self.correction_network.bert.encoder.layer[:2])\n",
    "        # 提取BERT的词向量大小\n",
    "        hidden_size = self.correction_network.bert.config.hidden_size\n",
    "\n",
    "        # 构造Detection Network\n",
    "        self.detection_network = DetectionNetwork(position_embeddings, transformer, hidden_size)\n",
    "\n",
    "    def forward(self, sequences, max_length=128):\n",
    "        # 先获取word embedding，Correction Network和Detection Network都要用\n",
    "        inputs, word_embeddings = self.correction_network.get_inputs_and_word_embeddings(sequences, max_length)\n",
    "        # Detection Network进行前向传递，获取输出的Hidden State和预测结果\n",
    "        hidden_states, detection_outputs = self.detection_network(word_embeddings)\n",
    "        # Correction Network进行前向传递，获取其预测结果\n",
    "        correction_outputs = self.correction_network(inputs, word_embeddings, hidden_states)\n",
    "        # 返回Correction Network 和 Detection Network 的预测结果。\n",
    "        # 在计算损失时`[PAD]`token不需要参与计算，所以这里将`[PAD]`部分全都变为0\n",
    "        return correction_outputs, detection_outputs.squeeze(2) * inputs['attention_mask']\n",
    "\n",
    "    def _init_correction_dense_layer(self):\n",
    "        \"\"\"\n",
    "        原论文中提到，使用Word Embedding的weight来对Correction Network进行初始化\n",
    "        \"\"\"\n",
    "        self.correction_network.dense_layer.weight.data = self.correction_network.word_embedding_table.weight.data\n",
    "        # pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "定义好模型后，我们来简单的尝试一下："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-roberta-wwm-ext were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction_outputs shape: torch.Size([2, 128, 21128])\n",
      "detection_outputs shape: torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "model = MDCSpellModel()\n",
    "correction_outputs, detection_outputs = model([\"鸡你太美\", \"哎呦，你干嘛！\"])\n",
    "print(\"correction_outputs shape:\", correction_outputs.size())\n",
    "print(\"detection_outputs shape:\", detection_outputs.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 损失函数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MDCSpellLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, coefficient=0.85):\n",
    "        super(MDCSpellLoss, self).__init__()\n",
    "        correction_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "        detection_criterion = nn.BCELoss()\n",
    "        self.coefficient = coefficient\n",
    "\n",
    "    def forward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "outputs": [],
   "source": [
    "correction_criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "detection_criterion = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected target size [2, 21128], got [2, 128]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-192-267d02ce3c3d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mcorrection_criterion\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcorrection_outputs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcorrection_targets\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input, target)\u001B[0m\n\u001B[0;32m   1162\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1163\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1164\u001B[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001B[0m\u001B[0;32m   1165\u001B[0m                                \u001B[0mignore_index\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mignore_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduction\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreduction\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1166\u001B[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001B[0m in \u001B[0;36mcross_entropy\u001B[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001B[0m\n\u001B[0;32m   3012\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0msize_average\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mreduce\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3013\u001B[0m         \u001B[0mreduction\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlegacy_get_string\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msize_average\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mreduce\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 3014\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_C\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_nn\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcross_entropy_loss\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_Reduction\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget_enum\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mreduction\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mignore_index\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel_smoothing\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   3015\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   3016\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Expected target size [2, 21128], got [2, 128]"
     ]
    }
   ],
   "source": [
    "correction_criterion(correction_outputs.view(), correction_targets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "correction_targets = tokenizer([\"鸡你太美\", \"哎呦，你干嘛！\"], padding='max_length', max_length=128, return_tensors='pt', truncation=True)['input_ids']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 128, 21128])"
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_outputs.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 128])"
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correction_targets.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}