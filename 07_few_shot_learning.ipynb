{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Pytorch入门实战（8）：小样本学习实现图片分类（Few-shot Learning, Meta Learning）"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iioSnail/pytorch_deep_learning_examples/blob/master/07_few_shot_learning.ipynb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 本文内容涉及知识点"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. [小样本学习的基本概念](https://blog.csdn.net/zhaohongfei_358/article/details/124057980)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 本文内容\n",
    "\n",
    "本文会使用Omniglot数据集训练一个孪生网络（相似网络），其可以用来判断两个图片的相似程度，通过该方式来实现小样本学习。\n",
    "\n",
    "本文使用Omniglot的训练集来训练神经网络，使用其验证集来构造Support Set。本文会从验证集的每个类别中拿出5个样本作为Support Set，即为5-shot的小样本学习。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 环境配置"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "本文所使用到的环境如下:\n",
    "\n",
    "```\n",
    "python==3.8.5\n",
    "torch==1.10.2\n",
    "torchvision==0.11.3\n",
    "numpy==1.22.3\n",
    "matplotlib==3.2.2\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "导入本文需要使用到的包："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 加载数据集"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们这里使用Pytorch提供的`torchvision.datasets.Omniglot`的方法来加载数据集。\n",
    "\n",
    "> 如果你用pytorch下载失败，可以使用[百度网盘链接](链接：https://pan.baidu.com/s/1I3UGUjqEgGKVfxgZEt5H6w?pwd=v6pu)下载，然后解压到当前文件夹下，将`download`参数改为`False`即可"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.Omniglot('./dataset', background=True, transform=transforms.ToTensor(), download=True)\n",
    "validation_dataset = torchvision.datasets.Omniglot('./dataset', background=False, transform=transforms.ToTensor(), download=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "成功加载完数据集后我们来简单看一下："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size: torch.Size([1, 105, 105])\n",
      "target: 0\n"
     ]
    }
   ],
   "source": [
    "image, target = train_dataset.__getitem__(0)\n",
    "print(\"image size:\", image.size())\n",
    "print(\"target:\", target)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Omniglot数据集都是一张一张的灰度图，和MNIST手写数据集差不多。这个target就是用数字表示的类别，我也不知道对应成实际类别是什么，也不需要知道。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们来简单绘制一张看一下："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x250e47ff460>"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAD7CAYAAACBpZo1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWcklEQVR4nO3df2zV9b3H8ee7v4Ei5UepWCgtyK/ZTTGNghuoOO9lWIfZLosaDd5rwpKholmygSPZWOJihLGRTa6pQ2W6CUQNOObYTGG5SEK1KBGhsrb8slBoBQFpS2np+/7Ro6vS2tPzo99T+nok5PR8z/f7Pa/0x4vv+Z7P+XzN3RGR/i0p6AAiEjwVgYioCERERSAiqAhEBBWBiBDHIjCz2Wa238yqzGxxvJ5HRKJn8RhHYGbJwL+A24Ea4B3gHnffF/MnE5GopcRpvzcAVe5+AMDM1gFzgU6LYMSIEZ6fnx+nKCICsGvXro/dPbuzx+JVBLnARx3u1wA3dlzBzBYACwDy8vIoLy+PUxQRATCzw109Fq9zBNbJsi+8BnH3Encvcvei7OxOS0pEekm8iqAGGNPh/mjgWJyeS0SiFK8ieAeYYGYFZpYG3A28HqfnEpEoxeUcgbu3mtlDwN+BZOA5d98bj+cSkejF62Qh7v4G8Ea89i8isaORhSKiIhARFYGIoCIQEVQEIoKKQERQEYgIKgIRQUUgIqgIRAQVgYigIhARVAQigopARFARiAgqAhFBRSAiqAhEBBWBiKAiEBFUBCKCikBEUBGICCoCEUFFICKoCEQEFYGIoCIQEVQEIoKKQERQEYgIKgIRQUUgIkRRBGY2xsy2mVmFme01s0Wh5cPM7E0zqwzdDo1dXBGJh2iOCFqBH7v7FGAasNDMvgYsBkrdfQJQGrovIgks4iJw91p3fzf09adABZALzAXWhlZbC9wVZUYRibOYnCMws3xgKlAG5Lh7LbSXBTCyi20WmFm5mZXX19fHIoaIRCjqIjCzTOBV4FF3Pxvudu5e4u5F7l6UnZ0dbQwRiUJURWBmqbSXwJ/c/bXQ4hNmNir0+CigLrqIIhJv0bxrYMAaoMLdV3Z46HVgfujr+cCmyOOJSG9IiWLbbwL3A3vMbHdo2ePAk8AGM3sQOALMiyqhiMRdxEXg7m8B1sXDt0W6XxHpfRpZKCIqAhFREYgIKgIRQUUgIqgIRAQVgYigIhARVAQigopARFARiAgqAhFBRSAiqAhEBBWBiKAiEBFUBCKCikBEUBGICCoCEUFFICJEN535Zae1tZU333yT/fv3R72vESNGcOeddzJkyJAYJBOAc+fOsXnzZo4fPx50lLBMnjyZ22+/neTk5KCjdEtF0EFLSwsvvvgi69evj3pfhYWFTJs2TUUQQ6dPn2bVqlW8/fbbQUcJy7333sutt96qIuiL2traaGtri3o/J0+e5NVXXyUnJ+cLy9PS0pg5cyajR4+O+jn6o1j9fHpDVVUVf/zjHxk3bhwzZ84kNTU16EhdUhHEydGjR1m6dCntV4b7t6ysLF566SUVQT9QVlbGrl27KC4u5oYbblAR9Fetra2XLGtsbGT79u00NjZSVFSkQriMuTstLS2d/h4kGhVBL2toaGD58uUMHjyYkpISFYEkBBVBAJqbmwF49913yczMZMqUKSoECZTGEQSkubmZ3/zmN9x3331s27Yt6DjSz6kIAtTQ0MDJkyepqKhg586d1NbWBh1JYmjEiBHceOONTJ48OeHfQlQRBOzixYusXr2a73//+/zlL38JOo7E0IwZM1i3bh1LlixhwIABQcf5SjpH0EFSUhJ5eXkUFhZGvI+mpiYOHz7cozPFZ86c4ezZs1RWVrJnzx5Gjhx5yfgDgdTUVMaPH09jY2PY23z88cdhjUTMyMggPz+flJTY/UlMmjSJq666irS0tJjtM17M3aPbgVkyUA4cdfdiMxsGrAfygUPAD9z9k6/aR1FRkZeXl0eVIxbcnbq6Os6dOxfxPnbv3s2PfvQj6urqerzt8OHDycrK4uGHH2bRokURZ7hcXbx4kdra2s9PtoajpKSE5cuX093v+TXXXMPq1avJzc2NNubnBg8eTHZ29iVjSYJiZrvcvaizx2JRf4uACuCK0P3FQKm7P2lmi0P3fxqD54k7MyMnJyeq/40//fRTxo0bR2ZmJtA+bPnEiRNcuHCh221PnjzJyZMnqa6u5sCBAwwZMoThw4dHnOVyk5yc3ON3V8L9/qWnpzN27FjGjh0bSbQ+L6pzBGY2GrgD+EOHxXOBtaGv1wJ3RfMcfc2kSZN4/vnn2bhxIxs3bqSkpIT8/Pwe7WP9+vXMnTuXNWvWdPs/mUgsRHtE8FvgJ8DgDsty3L0WwN1rzWxkZxua2QJgAUBeXl6UMRLHgAEDmDx58uf3s7KyGDNmDKdPn+aTTz6hpaWl233U1dVRV1dHVVUVR48eZfDgwfrwksRVxEcEZlYM1Ln7rki2d/cSdy9y96Ls7OxIYyS8kSNH8utf/5o1a9YwadKkHm27efNm5s2bx+rVq/vEMFXpu6I5Ivgm8F0zmwNkAFeY2UvACTMbFToaGAX0/KzZZSQ9PZ1rr72WUaNGMXr0aGpqajh37lxYf9i1tbXU1tYyZcoUvUSQuIr4iMDdl7j7aHfPB+4Gtrr7fcDrwPzQavOBTVGnvAxkZWWxbNky1qxZw3XXXRd0HJEviMeAoieB282sErg9dL/fS0tL44YbbmDWrFl6J0ASTkxGT7j7P4F/hr4+CdwWi/2KSO/QEGMRURGIiIpARFARiAgqAhFBH0OWy9TWrVvZvn07b731lgZjhUFFIJelrVu38sQTTwQdo89QEUi/lpubS3FxMRMnTuSKK67ofoPLlIpA+rWCggJ+8YtfkJOTkzATiARBRdBLmpqaKC0t5cCBA9TU1PRo28rKSp577jmuvvpqbr755phOp9XfJSUlkZSU1K9LAFQEvebs2bOsWLGCHTt2cPHixR5tu2PHDnbu3Mn3vvc9pk+friKQmNNvVC9xd1pbWyOaV+CzbXtaICLh0jgCEVERiIiKQETQOYK4a25u5sMPP+Sjjz7izJkzQccR6ZSKIM5OnDjBY489xgcffMDp06eDjiPSKRVBnF28eJGTJ09SX18fdBSRLukcgYjoiCBeLly4QG1tLYcPHw7rcmciQVIRxMmRI0d45JFHqK6u5siRI0HHEflKKoI4OX/+PJWVlVRVVQUdReLkzJkzNDQ0dLteRkYGQ4cOTejPM6gIRCLQ1tbGs88+yyuvvNLtujNmzGDZsmUMHDiwF5JFRkUgEgF359ChQ5SVlXW77siRIxP+cyJ610BEVAQioiIQEXSOIOY+mzugpaVFs+dKn6EiiLGDBw/y+9//noMHD1JXVxd0HJGwqAhirL6+nvXr13Ps2LFu1zUzzIy2trZeSCbSNZ0jCEhqaioPPPAAK1asYNq0aUHHkX5ORRCQlJQU5syZw0MPPcQ111wTdBzp56J6aWBmWcAfgELAgf8B9gPrgXzgEPADd/8kmufpC6qrq9mwYQOVlZWcPXs26DgSZ0lJSRQXF3PllVd2u+6ECRNIT0/vhVSRi/YcwSpgi7v/l5mlAQOBx4FSd3/SzBYDi4GfRvk8Ce/AgQOsWLGCU6dOBR1FeoGZMXv2bGbPnh10lJiIuAjM7ApgJvAAgLtfAC6Y2VzgltBqa4F/chkXQVVVFW+88QZ79+6lqamp2/XT09MpLi5mwoQJTJw4sRcS9h/uzrZt23jvvffYuXNn0HH6lGiOCMYB9cDzZnYtsAtYBOS4ey2Au9ea2cjoYyauPXv2sHTpUs6dOxfWuIEBAwbw4IMPMnv2bMyMlpaWXkjZP7g7mzZt4ne/+53GcPRQNEWQAlwPPOzuZWa2ivaXAWExswXAAoC8vLwoYgSvra2tR794usRW/Lh7WD+LMWPGMGvWLAoLCxkwYEAvJEts0RRBDVDj7p99/OoV2ovghJmNCh0NjAI6HVXj7iVACUBRUZHqW3pVYWEhK1euZMiQISQnJwcdJ3ARF4G7Hzezj8xskrvvB24D9oX+zQeeDN1uiknSBHPo0CHKy8vZuXNnWJcxy8jIYPr06eTn54d1plniLyUlRSUQEu27Bg8Dfwq9Y3AA+G/axyZsMLMHgSPAvCifIyG99dZbLFy4kPPnz4c1J+GQIUNYunQpN910E2lpab2QUCR8URWBu+8Gijp56LZo9pvIDh8+TEVFBe+++y6NjY1hX9TUzEhLSyMjIyPOCfsfd2fPnj0cOXKEQ4cOBR2nT9JnDXpoy5YtPP744zQ1NUV0ZWOJvZaWFlavXs26devCegtXLqUi6KHm5mZOnz4d9geF0tPTmTJlCmPHjmXIkCFxTtd/NTY26pJyUVARxFlOTg4rV66ksLCQrKysoOOIdEpFEKYTJ05w/Phxampqwlo/IyOD/Px8CgoKyM3NJTs7O84JJRxZWVnk5eUxfvx4kpL0mbvPqAjCtGHDBlauXMnZs2fDelmQl5fHM888w/jx48nJyemFhBKOmTNn8tRTTzF06FAGDRoUdJyEoSLoxscff8wnn3xCdXV1j85Ip6WlkZuby+jRo+MXTnps0KBBjB07Vu/efImK4Cu4Oy+88AIvvPCCrmYslzUVQTeOHz/O3r17w14/NTWV4cOHk52dTUqKvr3SN+g3NcYmTpzIU089RV5eHqNGjQo6jkhYVARdOHfuHOfPn6exsTGs9VNTU8nMzCQ3N5frr78+Lp8nuHDhAqdOnepy0ExycjKDBw+Oavx8c3MzDQ0NfepjvC0tLTQ3Nwcdo09TEXTi4sWLPPvss/z1r3+lsrIyrG2+/vWvs3TpUsaMGcOwYcPikqusrIz777+/yz/0q666imXLllFQUBDxc2zdupVVq1b1qVGTbW1tVFRUBB2jT1MRfMn58+c5f/4877//PqWlpWFvN3z4cG6++ea4lQBAXV3dV14rIT8/n7q6uqjGLFRXV1NaWtqnikCipyLo4MKFCzz99NNs376d3bt3Bx2nx+rr61myZElUQ5kPHTqU8FfuldhTEYS0trbS1NREWVkZmzaFP4VCUlISKSkppKSkBD7rUENDA9u2bQs0g/RNKgKgqamJ1atXs2vXLt55550ebTt16lR++MMfUlBQoJFq0mepCGh/SVBaWsrf/va3Hm+bl5fHPffcQ2ZmZhySSax8dnk5fb6gcyoC6Rduuukm5s2bx+TJk0lNTQ06TsJRESSAoM8t9AeFhYUsXLhQoz27oO9KlPbt28cvf/nLbuchzMzM5N57771k6vbk5GTmzp3L2LFj2bJlCzt27IhnXJFOqQiitH//fpYvX97teldeeSXf+ta3LimCz66hd8cdd3Dq1CkVgQRCRZAgzIxbb721R4eux44dY+PGjTQ0NMQ8z8SJE5kzZ85l83r6xhtv1InCr6AiSCB33nknxcXFYa+/c+dOtm7dGpci+MY3vsETTzxx2VwFSOdhvpqKIMH05Bc2JyeH+++/Py6Tdl5//fWkpqbqD6ifUBH0YePGjeNXv/pVXD4pmJSUpEPpfkRFQPu0Yrfccsvnswy3tbVRXl5OdXV1sMHCoEt2SSyoCGi/VPmjjz76+aSkLS0tLFq0qE8UgUgsqAhCOo4DSE1NZerUqRw/fvyS9Q4fPsy+ffu63V9mZiZFRUWfn2wbNmxYXD+iLBINS4SZaIqKiry8vDzoGF/Q1NTU6cVNn3nmGR5//PFupzSfPHkyf/7znxk3bhzQ/pp74MCBOpSXwJjZLnfv7FqlOiLoyoABAzp96+zqq69m5syZ3RZBQUEB2dnZusyZ9Akqgh6aM2cOM2bM6Ha9lJQUlYD0GSqCHurqSEGkL9MbxSKiIhCRKIvAzB4zs71m9oGZvWxmGWY2zMzeNLPK0O3QWIUVkfiIuAjMLBd4BChy90IgGbgbWAyUuvsEoDR0X0QSWLQvDVKAAWaWAgwEjgFzgbWhx9cCd0X5HCISZxEXgbsfBVYAR4Ba4Iy7/wPIcffa0Dq1wMjOtjezBWZWbmblutKwSLCieWkwlPb//QuAq4BBZnZfuNu7e4m7F7l7UTRX5hGR6EXz0uDbwEF3r3f3FuA14CbghJmNAgjddn2NLhFJCNEUwRFgmpkNtPbZK24DKoDXgfmhdeYD4V82SEQCEfHIQncvM7NXgHeBVuA9oATIBDaY2YO0l8W8WAQVkfiJaoixu/8c+PmXFjfTfnQgIn2ERhaKiIpARFQEIoKKQERQEYgIKgIRQUUgIqgIRAQVgYigIhARVAQigopARFARiAgqAhFBRSAiqAhEBBWBiKAiEBFUBCKCikBEUBGICCoCEUFFICKoCEQEFYGIoCIQEVQEIoKKQERQEYgIKgIRQUUgIqgIRIQwisDMnjOzOjP7oMOyYWb2pplVhm6HdnhsiZlVmdl+M/vPeAUXkdgJ54jgBWD2l5YtBkrdfQJQGrqPmX0NuBu4JrTNajNLjllaEYmLbovA3f8POPWlxXOBtaGv1wJ3dVi+zt2b3f0gUAXcEJuoIhIvkZ4jyHH3WoDQ7cjQ8lzgow7r1YSWXcLMFphZuZmV19fXRxhDRGIh1icLrZNl3tmK7l7i7kXuXpSdnR3jGCLSE5EWwQkzGwUQuq0LLa8BxnRYbzRwLPJ4ItIbIi2C14H5oa/nA5s6LL/bzNLNrACYALwdXUQRibeU7lYws5eBW4ARZlYD/Bx4EthgZg8CR4B5AO6+18w2APuAVmChu1+MU3YRiZFui8Dd7+niodu6WP8J4IloQolI79LIQhFREYiIikBEUBGICGDunY736d0QZvVAA/Bx0FnCMILEz9kXMkLfyHk5ZRzr7p2O3kuIIgAws3J3Lwo6R3f6Qs6+kBH6Rs7+klEvDURERSAiiVUEJUEHCFNfyNkXMkLfyNkvMibMOQIRCU4iHRGISEBUBCKSGEVgZrNDk51WmdnioPMAmNkYM9tmZhVmttfMFoWWdzlxa4BZk83sPTPbnMAZs8zsFTP7MPQ9nZ5oOc3ssdDP+gMze9nMMhIhY29MIBx4EYQmN30a+A7wNeCe0CSoQWsFfuzuU4BpwMJQrk4nbg3YIqCiw/1EzLgK2OLuk4Frac+bMDnNLBd4BChy90IgmfaJeBMh4wvEewJhdw/0HzAd+HuH+0uAJUHn6iTnJuB2YD8wKrRsFLA/4FyjQ78Is4DNoWWJlvEK4CChk9MdlidMTv493+Yw2j+evxn4j0TJCOQDH3T3vfvy3w/wd2B6d/sP/IiAHkx4GhQzywemAmV0PXFrUH4L/ARo67As0TKOA+qB50MvYf5gZoNIoJzufhRYQftEO7XAGXf/RyJl/JKoJxDuKBGKIOwJT4NgZpnAq8Cj7n426DwdmVkxUOfuu4LO0o0U4Hrgf919Ku2fK0mElyufC73GngsUAFcBg8zsvmBTRSSiv6dEKIKEnfDUzFJpL4E/uftrocVdTdwahG8C3zWzQ8A6YJaZvURiZYT2n3GNu5eF7r9CezEkUs5vAwfdvd7dW4DXgJsSLGNHMZ1AOBGK4B1ggpkVmFka7Sc6Xg84E2ZmwBqgwt1Xdnioq4lbe527L3H30e6eT/v3bau730cCZQRw9+PAR2Y2KbToNtrntUyknEeAaWY2MPSzv432E5qJlLGj2E4gHNTJmS+dCJkD/AuoBn4WdJ5Qpm/Rfkj1PrA79G8OMJz2k3OVodthQWcN5b2Ff58sTLiMwHVAeej7uREYmmg5gWXAh8AHwItAeiJkBF6m/bxFC+3/4z/4VbmAn4X+lvYD3wnnOTTEWEQS4qWBiARMRSAiKgIRURGICCoCEUFFICKoCEQE+H80/wLkBjoctAAAAABJRU5ErkJggg==\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze(), cmap='gray')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据处理"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "在孪生网络中，我们一次给到模型的是一对儿图片，然后让模型来区分这对儿图片是否是相同的类别。我们本章就需要来定义这么一个函数，来生成一批样本对儿，其中一半图片对儿是相同类别，另一半图片对儿是不同类别。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们首先获取一下训练集中的所有target和所有labels："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "all_targets = np.array([train_dataset.__getitem__(i)[1] for i in range(len(train_dataset))])\n",
    "all_labels = np.array(list(set(all_targets)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_targets: [  0   0   0 ... 963 963 963]\n",
      "all_labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791\n",
      " 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809\n",
      " 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827\n",
      " 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845\n",
      " 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863\n",
      " 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881\n",
      " 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899\n",
      " 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917\n",
      " 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935\n",
      " 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953\n",
      " 954 955 956 957 958 959 960 961 962 963]\n"
     ]
    }
   ],
   "source": [
    "print(\"all_targets:\", all_targets)\n",
    "print(\"all_labels:\", all_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "准备好了这两个基础数据，我们就可以来构造我们的Sample函数了。其作用就是返回一个batch的图片对儿，其中一半是相同的类别，称为**正样本**，另一半是不同的类别，称为**负样本**。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def sample_batch(batch_size):\n",
    "    \"\"\"\n",
    "    从train_dataset中sample一些数据对。一半正样本，一半负样本\n",
    "    \"\"\"\n",
    "\n",
    "    # 选取二分之一个batch的labels作为正样本，这样就完成了正样本的构造。\n",
    "    positive_labels = np.random.choice(all_labels, batch_size // 2)\n",
    "    # 针对这些labels，每个选取两张相同类别的图片\n",
    "    batch = []\n",
    "    for label in positive_labels:\n",
    "        labels_indexes = np.argwhere(all_targets == label)\n",
    "        pair = np.random.choice(labels_indexes.flatten(), 2)\n",
    "        batch.append((pair[0], pair[1], 1)) # 图片类别相同，所以target为1\n",
    "\n",
    "    # 选取负样本，这次选取一个batch的labels，然后每个labels个选取一张图片。这样就完成了负样本的构造。\n",
    "    negative_labels = np.random.choice(all_labels, batch_size)\n",
    "    for sample1, sample2 in negative_labels.reshape(-1, 2):\n",
    "        sample1 = np.random.choice(np.argwhere(all_targets == sample1).flatten(), 1)\n",
    "        sample2 = np.random.choice(np.argwhere(all_targets == sample2).flatten(), 1)\n",
    "        batch.append((sample1.item(), sample2.item(), 0)) # 图片类别不相同，所以target为0\n",
    "\n",
    "    \"\"\"\n",
    "    完成上面的动作后，最终得到的batch如下：\n",
    "        (734, 736, 1),\n",
    "        (127, 132, 1),\n",
    "        ...\n",
    "        (859, 173, 0),\n",
    "        ...\n",
    "    其中前两个表示样本对对应在dataset中的index，1表示前两个样本是相同类别。0表示这两个样本为不同类别。\n",
    "    接下来需要对其进行shuffle处理，然后从dataset中获取到对应数据，最终组成batch.\n",
    "    \"\"\"\n",
    "    random.shuffle(batch)\n",
    "\n",
    "    sample1_list = []\n",
    "    sample2_list = []\n",
    "    target_list = []\n",
    "    for sample1, sample2, target in batch:\n",
    "        sample1_list.append(train_dataset.__getitem__(sample1)[0])\n",
    "        sample2_list.append(train_dataset.__getitem__(sample2)[0])\n",
    "        target_list.append(target)\n",
    "    sample1 = torch.stack(sample1_list)\n",
    "    sample2 = torch.stack(sample2_list)\n",
    "    targets = torch.LongTensor(target_list)\n",
    "    return sample1, sample2, targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "完成sample函数后，我们来简单试一下："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "sample1, sample2, targets = sample_batch(16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample1: torch.Size([16, 1, 105, 105])\n",
      "sample2: torch.Size([16, 1, 105, 105])\n",
      "targets: tensor([1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "print(\"sample1:\", sample1.size())\n",
    "print(\"sample2:\", sample1.size())\n",
    "print(\"targets:\", targets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "其中`sample1[0]`和`smaple2[0]` 是一对儿，`targets[0]`是它们的标签，表示它们是否是相同类别。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "准备好数据后，我们接下来开始构建模型："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们需要构建的模型也很简单，模型的功能就是输入两张图片，输出这两张图片是否为同一个类别。由于是二分类问题，所以我们最后的值通过Sigmoid处理一下："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class SimilarityModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimilarityModel, self).__init__()\n",
    "\n",
    "        # 定义一个卷积层，用于特征提取。\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 4, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(4, 16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # 定义一个线性层，用来判断两张图片是否为同一类别\n",
    "        self.sim = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2592, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, sample1, sample2):\n",
    "        # 使用卷积层提取sample1的特征\n",
    "        sample1_features = self.conv(sample1)\n",
    "        # 使用卷积层提取sample2的特征\n",
    "        sample2_features = self.conv(sample2)\n",
    "        # 将 |sample1-sample2| 的结果送给线性层，判断它们的相似度。\n",
    "        return self.sim(torch.abs(sample1_features - sample2_features))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "model = SimilarityModel()\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "模型定义完后，我们来简单的尝试一下："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.4891],\n        [0.4893],\n        [0.4896],\n        [0.4895],\n        [0.4893],\n        [0.4890],\n        [0.4890],\n        [0.4892],\n        [0.4892],\n        [0.4891],\n        [0.4893],\n        [0.4890],\n        [0.4893],\n        [0.4897],\n        [0.4891],\n        [0.4896]], grad_fn=<SigmoidBackward0>)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(sample1.to(device), sample2.to(device))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到，由于模型还未训练，所以输出的值都在0.5%左右。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 训练模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来开始训练模型，和普通的二分类问题差别不算很大。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "model = model.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "criteria = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, loss 0.6934548616409302\n",
      "episode 50, loss 0.6494089365005493\n",
      "episode 100, loss 0.592072069644928\n",
      "episode 150, loss 0.6408461928367615\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-24-4a80ef402761>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[1;31m# 将模型的结果丢给BCELoss计算损失\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[0mloss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcriteria\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutputs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mflatten\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtargets\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 16\u001B[1;33m     \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     17\u001B[0m     \u001B[1;31m# 更新模型参数\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    305\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    306\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 307\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    308\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    309\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    152\u001B[0m         \u001B[0mretain_graph\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 154\u001B[1;33m     Variable._execution_engine.run_backward(\n\u001B[0m\u001B[0;32m    155\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    156\u001B[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "# 如果500次迭代loss都没有下降，那么就停止训练\n",
    "early_stop = 500\n",
    "# 记录最小的一次loss值\n",
    "min_loss = 100.\n",
    "# 记录下上一次最小的loss是哪一次\n",
    "last_episode = 0\n",
    "# 无线更新参数，直到loss不再下降为止\n",
    "for episode in range(100000):\n",
    "    # 使用sample_batch函数sample出一组数据，包含一半正样本，一半负样本\n",
    "    sample1, sample2, targets = sample_batch(batch_size)\n",
    "    # 将数据送入模型，判断是否为同一类别\n",
    "    outputs = model(sample1.to(device), sample2.to(device))\n",
    "    # 将模型的结果丢给BCELoss计算损失\n",
    "    loss = criteria(outputs.flatten(), targets.to(device).float())\n",
    "    loss.backward()\n",
    "    # 更新模型参数\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 如果本次损失比之前的小，那么记录一下\n",
    "    if loss < min_loss:\n",
    "        min_loss = loss.item()\n",
    "        last_episode = episode\n",
    "        torch.save(model, 'best_model.pt')\n",
    "\n",
    "    # 如果连续{early_stop}次loss都没有减小，那么就停止训练\n",
    "    if episode - last_episode > early_stop:\n",
    "        break\n",
    "\n",
    "    # 每50个episode打印一下日志\n",
    "    if episode % 50 ==  0:\n",
    "        print(f\"episode {episode}, loss {loss}\")\n",
    "\n",
    "print(\"Finish Training.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 模型验证"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "验证前先加载一下之前最好的模型："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-50-eca23e4adc85>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmodel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'best_model.pt'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001B[0m\n\u001B[0;32m    592\u001B[0m         \u001B[0mpickle_load_args\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'encoding'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'utf-8'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    593\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 594\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'rb'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mopened_file\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    595\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0m_is_zipfile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    596\u001B[0m             \u001B[1;31m# The zipfile reader is going to advance the current file position.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    229\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0m_is_path\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 230\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    231\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    232\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;34m'w'\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_opener\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_open_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__exit__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'best_model.pt'"
     ]
    }
   ],
   "source": [
    "model = torch.load('best_model.pt')\n",
    "model = model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "接下来我们来验证下我们模型的表现。我们使用上面构造好的validation_dataset。在validation_dataset中，全都是模型之间没有见过的类别，可不是没有见过的图片哦，连这个类别它都没有见过。不信你可以到`dataset/images_evaluation`这个目录下表看看那。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们先来看下验证集中的类别情况："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "all_targets = np.array([validation_dataset.__getitem__(i)[1] for i in range(len(validation_dataset))])\n",
    "all_labels = np.array(list(set(all_targets)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size: 13180\n",
      "all_targets: [  0   0   0 ... 658 658 658]\n",
      "all_labels: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658]\n"
     ]
    }
   ],
   "source": [
    "print(\"sample size:\", len(all_targets))\n",
    "print(\"all_targets:\", all_targets)\n",
    "print(\"all_labels:\", all_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到，在验证集中我们一共有13180个数据，有658个类别，每个类别有20个样本。我们为每个类别选取5个样本作为support set供模型参考，剩下的15个作为验证集来验证模型的表现。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "support_set = []\n",
    "validation_set = []\n",
    "# 遍历所有的标签，每个标签选取前5个作为support set，后面的作为验证数据\n",
    "for label in all_labels:\n",
    "    label_indexes = np.argwhere(all_targets == label)\n",
    "    support_set.append((label_indexes[:5].flatten().tolist()))\n",
    "    validation_set += label_indexes[5:].flatten().tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support set: [[0, 1, 2, 3, 4], [20, 21, 22, 23, 24], [40, 41, 42, 43, 44], [60, 61, 62, 63, 64], [80, 81, 82, 83, 84]]\n",
      "validation set: [5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "print(\"support set:\", support_set[:5])\n",
    "print(\"validation set:\", validation_set[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们接下来需要定义一个预测函数，其给定一个图片，输出该图片的target。该函数的思路为：让该图片与support set里的所有类别都比较一下，看看与谁的相似度最高，那么该图片就是什么类别。由于一个类别有5张图片，我们可以通过取平均的方式来得到该图片为该类别的可能性。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def predict(image):\n",
    "    sim_list = [] # 存储image与每个类别的相似度\n",
    "    # 一个类别一个类别的遍历，indexes存储的就是当前类别的5张图片的index\n",
    "    for indexes in support_set:\n",
    "        # 去validation_dataset中找出index对应的图片tensor\n",
    "        tensor_list = []\n",
    "        for i in indexes:\n",
    "            tensor_list.append(validation_dataset[i][0])\n",
    "        support_tensor = torch.stack(tensor_list)\n",
    "        # 拿到该类别的5个图片后，就可以送给模型求image与它们的相似程度了，最后求个平均\n",
    "        sim = model(image.repeat(5, 1, 1, 1).to(device), support_tensor.to(device)).mean()\n",
    "        sim_list.append(sim)\n",
    "\n",
    "    # 找出其中相似程度最高的那个，它就是预测结果\n",
    "    result_index = torch.stack(sim_list).argmax().item()\n",
    "    return all_labels[result_index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "我们来试下predict函数："
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.68 s ± 154 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "predict(validation_dataset.__getitem__(validation_set[0])[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "可以看到这个函数速度非常慢，这是因为类别太多了，每个类别都要过一次模型。大家可以思考思考应该如何优化。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "最后我们一个个的对验证集的数据进行验证，计算正确率。"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/9885 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a4366ce2ee84e339b16f41fc238c8fd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-385-fbd12db0082e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mprogress\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mimage\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlabel\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mvalidation_dataset\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m     \u001B[0mpredict_label\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mtotal\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-350-55f54f5f65c4>\u001B[0m in \u001B[0;36mpredict\u001B[1;34m(image)\u001B[0m\n\u001B[0;32m      6\u001B[0m             \u001B[0mtensor_list\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mvalidation_dataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[0msupport_tensor\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstack\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 8\u001B[1;33m         \u001B[0msim\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrepeat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m5\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msupport_tensor\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      9\u001B[0m         \u001B[0msim_list\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msim\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-233-029e3d1cc016>\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, sample1, sample2)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 29\u001B[1;33m         \u001B[0msample1_features\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msample1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     30\u001B[0m         \u001B[0msample2_features\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msample2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     31\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mabs\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msample1_features\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0msample2_features\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    139\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    140\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 141\u001B[1;33m             \u001B[0minput\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    142\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    143\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1100\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1103\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1104\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    444\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    445\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mTensor\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 446\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_conv_forward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    447\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    448\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0mConv3d\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_ConvNd\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001B[0m in \u001B[0;36m_conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    440\u001B[0m                             \u001B[0mweight\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mbias\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstride\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    441\u001B[0m                             _pair(0), self.dilation, self.groups)\n\u001B[1;32m--> 442\u001B[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001B[0m\u001B[0;32m    443\u001B[0m                         self.padding, self.dilation, self.groups)\n\u001B[0;32m    444\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "total_correct = 0\n",
    "\n",
    "progress = tqdm(validation_set)\n",
    "\n",
    "for i in progress:\n",
    "    image, label = validation_dataset.__getitem__(i)\n",
    "    predict_label = predict(image)\n",
    "\n",
    "    total += 1\n",
    "    if predict_label == label:\n",
    "        total_correct += 1\n",
    "\n",
    "    progress.set_postfix({\n",
    "            \"accuracy\": str(\"%.3f\" % (total_correct / total))\n",
    "        })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_correct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-40-b8eac5b439ac>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Accuracy:\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtotal_correct\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mtotal\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m: name 'total_correct' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", total_correct / total)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}